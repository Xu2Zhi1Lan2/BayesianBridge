% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/BBLR.R
\name{BBLR}
\alias{BBLR}
\title{Bayesian Bridge Linear Regression}
\usage{
BBLR(
  y,
  X,
  step_sizes_tuning_iterations,
  Tb,
  burn,
  nmc,
  thin,
  method.alpha,
  alpha_sig = 0.05
)
}
\arguments{
\item{y}{A numeric vector of the response variable with length n.}

\item{X}{A numeric matrix with n rows (observations) and p columns (predictors).}

\item{step_sizes_tuning_iterations}{The number of iterations dedicated to tuning the MCMC sampling algorithm for optimal step sizes.}

\item{Tb}{The tuning block size, specifying how often the algorithm adjusts the step sizes.}

\item{burn}{The number of initial MCMC iterations to discard, allowing the algorithm to reach stationary distribution.}

\item{nmc}{The total number of MCMC iterations to perform post-tuning and burn-in, for final analysis.}

\item{thin}{Thinning interval for the MCMC sampling to reduce autocorrelation in the chain.}

\item{method.alpha}{Specifies the method to update the alpha parameter. It can be "fixed" for a constant alpha throughout the sampling, or "beta" to sample alpha from a Beta distribution.}

\item{alpha_sig}{Significance level used for computing credible intervals of the estimated coefficients.}
}
\value{
A list containing the mean estimated coefficients (BetaHat), their credible intervals (LeftCI, RightCI), the median of the coefficient estimates (BetaMedian), the estimated variance of the error term (Sigma2Hat), the estimated parameter of the bridge prior (LambdaHat), the estimated shape parameter of the bridge prior (AlphaHat), and samples from the posterior distributions of the coefficients (BetaSamples), error variance (Sigma2Samples), lambda (LambdaSamples), and alpha (AlphaSamples).
}
\description{
This function implements the Bayesian bridge linear regression, a statistical method for linear regression analysis in high-dimensional data sets.
}
\examples{
\donttest{
# Assuming y is your response variable and X is your matrix of predictors
result <- BBLR(y, X, tuning=1000, Tb=100, burn=500, nmc=5000, thin=5, method.alpha="beta")
}
}
